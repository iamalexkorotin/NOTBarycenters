{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0f316f4",
   "metadata": {},
   "source": [
    "# Ave, Celeba! Experiment for latent space with entropy regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817d9603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "from src.utils import Config, weights_init_D, freeze, unfreeze, normalize_out_to_0_1\n",
    "from src.data import DatasetSampler\n",
    "from src.models import ResNet_D, UNet, linear_model\n",
    "from src.train_entropic import train_entropic\n",
    "from src.cost import strong_cost\n",
    "\n",
    "# for generator\n",
    "import sys \n",
    "sys.path.append(\"..\")\n",
    "import dnnlib\n",
    "import legacy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14ec535",
   "metadata": {},
   "source": [
    "## 1. Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dabcf7",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 1 \n",
    "INNER_ITERATIONS = 10\n",
    "BATCH_SIZE =32\n",
    "EPSILON = 1.\n",
    "LR_LATENT_MLP=1e-4\n",
    "LR_ENCODER=1e-4\n",
    "LR_POTENTIAL = 1e-4\n",
    "GPU_DEVICE = 0\n",
    "ALAE = False\n",
    "RESNET_D_ENCODER = True\n",
    "CHI = False\n",
    "SPHERE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f187a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = Config()\n",
    "\n",
    "CONFIG.K = 3\n",
    "CONFIG.EPSILON = EPSILON\n",
    "\n",
    "CONFIG.LAMBDAS = [0.25,0.5,0.25]\n",
    "CONFIG.CLASSES = [0,1,2]\n",
    "CONFIG.DATASET_PATH ='../../../data/ave_celeba_green_v2/' \n",
    "CONFIG.DATASET = 'ave_celeba'\n",
    "\n",
    "CONFIG.SPHERE_PROJECTION = SPHERE\n",
    "CONFIG.CHI_PROJECTION = CHI\n",
    "CONFIG.FLAG_LATENT = True\n",
    "CONFIG.GENERATOR_PATH =  \"../../../stylegan2_ada_pytorch_before/training-runs\\\n",
    "/00011-aligned_celeba-stylegan2/network-snapshot-008800.pkl\"\n",
    "CONFIG.LATENT_ENCODER_SIZE =256\n",
    "CONFIG.LATENT_SIZE = 512\n",
    "CONFIG.ALAE = ALAE\n",
    "CONFIG.RESNET_D_ENCODER = RESNET_D_ENCODER\n",
    "\n",
    "CONFIG.BATCH_SIZE =BATCH_SIZE\n",
    "CONFIG.IMG_SIZE = 64\n",
    "CONFIG.NC = 3\n",
    "CONFIG.NUM_EPOCHS = NUM_EPOCHS\n",
    "CONFIG.INNER_ITERATIONS = INNER_ITERATIONS\n",
    "\n",
    "\n",
    "CONFIG.HIDDEN_SIZE = [max(2*CONFIG.LATENT_SIZE,128),\n",
    "                      max(2*CONFIG.LATENT_SIZE,128)] \n",
    "\n",
    "CONFIG.LR_LATENT_MLP = LR_LATENT_MLP\n",
    "\n",
    "CONFIG.LR_ENCODER  = LR_ENCODER\n",
    "\n",
    "CONFIG.LR_POTENTIAL = LR_POTENTIAL\n",
    "CONFIG.BETAS = (0.2, 0.99)\n",
    "\n",
    "CONFIG.GPU_DEVICE = GPU_DEVICE\n",
    "assert torch.cuda.is_available()\n",
    "CONFIG.DEVICE = f'cuda:{CONFIG.GPU_DEVICE}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cec5f9",
   "metadata": {},
   "source": [
    "## 2. Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e168a065",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG.FLAG_LATENT:\n",
    "    with dnnlib.util.open_url(CONFIG.GENERATOR_PATH) as f:\n",
    "        G =  legacy.load_network_pkl(f)['G_ema'].to(CONFIG.DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6be22c",
   "metadata": {},
   "source": [
    "## 3. Potential and Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f17f5b4",
   "metadata": {},
   "source": [
    "### 3.1 potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0868f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "nets_for_pot = [ResNet_D(size=CONFIG.IMG_SIZE,\n",
    "                  nc=CONFIG.NC,\n",
    "                  nfilter=64, \n",
    "                  nfilter_max=512, \n",
    "                  res_ratio=0.1,\n",
    "                  n_output=1,bn_flag=False,pn_flag=False).to(CONFIG.DEVICE)\n",
    "                for i in range(CONFIG.K)]\n",
    "\n",
    "\n",
    "for f in nets_for_pot: \n",
    "    weights_init_D(f)\n",
    "    \n",
    "param_nets = [net.parameters() for net in nets_for_pot]\n",
    "nets_for_pot_opt = torch.optim.Adam(itertools.chain(*param_nets),\n",
    "                               CONFIG.LR_POTENTIAL, betas=CONFIG.BETAS)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230288c2",
   "metadata": {},
   "source": [
    "### 3.2 Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4c618c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder = ResNet_D(size=CONFIG.IMG_SIZE,\n",
    "              nc=CONFIG.NC,\n",
    "              nfilter=64, \n",
    "              nfilter_max=512, \n",
    "              res_ratio=0.1,\n",
    "              n_output=2*CONFIG.LATENT_SIZE,bn_flag=True,pn_flag=True).to(CONFIG.DEVICE)\n",
    "\n",
    "weights_init_D(encoder)\n",
    "\n",
    "encoder_opt = torch.optim.Adam(encoder.parameters(),\n",
    "                              CONFIG.LR_ENCODER, betas=CONFIG.BETAS)\n",
    "\n",
    "latent_mlp, latent_mlp_opt = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3663b690",
   "metadata": {},
   "source": [
    "## 4. Data Samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a9f6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(CONFIG.IMG_SIZE),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Lambda(lambda x: torch.clip(x,0,1))\n",
    "])\n",
    "\n",
    "data_samplers=[]\n",
    "for k in tqdm(range(CONFIG.K)):\n",
    "    dataset = torchvision.datasets.ImageFolder(os.path.join(CONFIG.DATASET_PATH,f\"ave_celeba_{k}/\"), transform=transform)\n",
    "    data_samplers.append(DatasetSampler(dataset, flag_label=True, batch_size=256 ,num_workers=40))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97b10b7",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2b481e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "name_exp = f\"KL_EPS_{EPSILON}_ALAE_{ALAE}_ENC_{RESNET_D_ENCODER}_EPS_{EPSILON}\"\n",
    "CONFIG.NAME_EXP = name_exp\n",
    "wandb.init(project=\"BNOT\" ,\n",
    "           name=name_exp, \n",
    "           config=CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b364a7dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_entropic(nets_for_pot, \n",
    "      nets_for_pot_opt,\n",
    "           encoder,encoder_opt,\n",
    "           latent_mlp=latent_mlp, latent_mlp_opt=latent_mlp_opt,\n",
    "          data_samplers=data_samplers,\n",
    "          generator=G,\n",
    "          config=CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47846244",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
