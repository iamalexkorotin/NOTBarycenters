{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ebdfd9f",
   "metadata": {},
   "source": [
    "# SHAPE COLOR EXPERIMENT ENTROPIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699bd721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from src.utils import Config, weights_init_D, freeze, unfreeze, normalize_out_to_0_1, middle_rgb\n",
    "from src.data import DatasetSampler\n",
    "from src.models import ResNet_D, UNet, linear_model\n",
    "from src.train_shape_color_entropic import train_shape_color_entropic\n",
    "from src.cost import strong_cost, cost_image_color_latent, cost_image_shape_latent\n",
    "\n",
    "# for generator\n",
    "import sys \n",
    "sys.path.append(\"..\")\n",
    "import dnnlib\n",
    "import legacy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab83735",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 1\n",
    "INNER_ITERATIONS = 10\n",
    "BATCH_SIZE = 64\n",
    "EPSILON = 0.1\n",
    "LR_LATENT_MLP=1e-4\n",
    "LR_ENCODER=1e-4\n",
    "LR_POTENTIAL = 1e-4\n",
    "GPU_DEVICE = 0\n",
    "ALAE=False\n",
    "RESNET_D_ENCODER = True\n",
    "CHI=False\n",
    "SPHERE=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910120ad",
   "metadata": {},
   "source": [
    "## 1. Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a83a211",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = Config()\n",
    "\n",
    "CONFIG.K = 2\n",
    "CONFIG.CLASSES = [2]\n",
    "CONFIG.LAMBDAS = [.5,.5]\n",
    "\n",
    "CONFIG.EPSILON = EPSILON\n",
    "CONFIG.NUM_EPOCHS = NUM_EPOCHS\n",
    "CONFIG.INNER_ITERATIONS = INNER_ITERATIONS\n",
    "CONFIG.BATCH_SIZE = BATCH_SIZE\n",
    "\n",
    "CONFIG.DATASET = 'colored_mnist'\n",
    "CONFIG.IMG_SIZE = 32\n",
    "CONFIG.NC = 1\n",
    "CONFIG.DATASET_PATH = \"../../../data/MNIST\"\n",
    "\n",
    "\n",
    "CONFIG.FLAG_LATENT = True\n",
    "CONFIG.GENERATOR_PATH = \"../../../stylegan2_ada_pytorch_before/ckpts/MNIST-colored_2_3/network-snapshot-002000.pkl\"\n",
    "CONFIG.LATENT_SIZE =512\n",
    "\n",
    "\n",
    "CONFIG.LR_LATENT_MLP = LR_LATENT_MLP\n",
    "CONFIG.LR_ENCODER  = LR_ENCODER\n",
    "CONFIG.LR_POTENTIAL = LR_POTENTIAL\n",
    "CONFIG.BETAS = (0.2, 0.99)\n",
    "\n",
    "\n",
    "CONFIG.NUMBER_PALETTES = 50_000\n",
    "CONFIG.HUE_MEAN = 120 # for green color: diapasone from 0 to 360\n",
    "CONFIG.HUE_MEANS = [0,60,120]\n",
    "CONFIG.HUE_STD = 0.\n",
    "CONFIG.SATURATION = 1 # from 0 to 1\n",
    "CONFIG.BRIGHTNESS = 1 # from 0 to 1\n",
    "CONFIG.SATURATION_THRESHOLD = 0.8 # from 0 to 1\n",
    "\n",
    "CONFIG.GPU_DEVICE = GPU_DEVICE\n",
    "assert torch.cuda.is_available()\n",
    "CONFIG.DEVICE = f'cuda:{CONFIG.GPU_DEVICE}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36216224",
   "metadata": {},
   "source": [
    "## 2. Style GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c2d50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG.FLAG_LATENT:\n",
    "    with dnnlib.util.open_url(CONFIG.GENERATOR_PATH) as f:\n",
    "        G =  legacy.load_network_pkl(f)['G_ema'].to(CONFIG.DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14a493e",
   "metadata": {},
   "source": [
    "## 3. Make dataset of palettes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518eb0a0",
   "metadata": {},
   "source": [
    "Here, we make rgb dataset $\\mathbb{D}$:\n",
    "\n",
    "- $\\forall x \\in \\mathbb{D} \\to x \\in \\mathbb{R}^{3}$\n",
    "- $\\forall x \\in \\mathbb{D} \\to x[0] \\in [0,1]$ is \\textbf{hue}, $x[1],x[2] \\in [0,1]$ are $\\textbf{saturation}$ and $\\textbf{brightness}$ correspondingly.\n",
    "\n",
    "Creating:\n",
    "- Set mean of $\\textbf{Hue}$ to 120 as middle value for green color\n",
    "- Define std and number of items for dataset\n",
    "- Define $\\textbf{saturation}$ and $\\textbf{brightness}$ \n",
    "- Stack in one  ndarray\n",
    "- Normalize $\\textbf{Hue}$ from 0 to 360 - from 0 to 1 , dividing by 360.\n",
    "- Use matplotlib.function for translation hsv vector to rgb\n",
    "- Build palette color map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c273f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize hue spectr for green color, satiration and brightness\n",
    "hue_vectors = CONFIG.HUE_MEAN + np.random.randn(CONFIG.NUMBER_PALETTES)*CONFIG.HUE_STD # shape:( NUMBER_PALETTES, )\n",
    "#hue_vectors = np.random.randint(low=0, high=131 ,size=CONFIG.NUMBER_PALETTES)\n",
    "saturation_vectors = CONFIG.SATURATION*np.ones(CONFIG.NUMBER_PALETTES) # shape:( NUMBER_PALETTES, )\n",
    "brightness_vectors = CONFIG.BRIGHTNESS*np.ones(CONFIG.NUMBER_PALETTES) # shape:( NUMBER_PALETTES, )\n",
    "\n",
    "# create HSV dataset\n",
    "hsv_vectors = np.stack([hue_vectors.reshape(-1,1),\n",
    "                        saturation_vectors.reshape(-1,1),\n",
    "                        brightness_vectors.reshape(-1,1)],axis=1).reshape(-1, 3)# shape:(NUMBER_PALETTES,3)\n",
    "\n",
    "# translate HSV -> RGB \n",
    "# Importantly: now Hue from 0 to 360 and we translate it from 0 to 1\n",
    "hsv_vectors[:,0] = hsv_vectors[:,0]/360\n",
    "\n",
    "# we use matplotlib function : https://matplotlib.org/stable/api/_as_gen/matplotlib.colors.hsv_to_rgb.html \n",
    "rgb_dataset = matplotlib.colors.hsv_to_rgb(hsv_vectors)\n",
    "assert rgb_dataset.shape == (CONFIG.NUMBER_PALETTES, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9a36f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot palette of RGB dataset\n",
    "N = 1000\n",
    "x = np.random.rand(N)\n",
    "y = np.random.rand(N)\n",
    "c = rgb_dataset[:1000]\n",
    "\n",
    "plt.scatter(x, y, c=c, label=\"RGB dataset\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5165b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_sampler = DatasetSampler(rgb_dataset,flag_label=False,batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c24d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PLOT_GENERATED = 20\n",
    "fake = G(torch.randn(NUM_PLOT_GENERATED,512).to(CONFIG.DEVICE),c=None)\n",
    "clr = middle_rgb(normalize_out_to_0_1(fake, CONFIG), CONFIG.SATURATION_THRESHOLD )\n",
    "\n",
    "# plot images from Style-GAN\n",
    "fig,ax = plt.subplots(1,NUM_PLOT_GENERATED,figsize=(10,20),dpi=150)\n",
    "for i in range(NUM_PLOT_GENERATED):\n",
    "    ax[i].imshow(normalize_out_to_0_1(fake[i], CONFIG).permute(1,2,0).detach().cpu())\n",
    "    ax[i].set_xticks([]);ax[i].set_yticks([]);\n",
    "fig.tight_layout(pad=0.01)\n",
    "\n",
    "# plot palettes\n",
    "figure, axes = plt.subplots(1,20,figsize=(10,20),dpi=150) \n",
    "for i in range(NUM_PLOT_GENERATED): \n",
    "    axes[i].set_aspect( 1 ) \n",
    "    axes[i].add_artist(plt.Circle(( 0.5 , 0.5 ), 0.4 ,color=clr[i].cpu().numpy()) ) \n",
    "    axes[i].set_xticks([]);axes[i].set_yticks([]);\n",
    "fig.tight_layout(pad=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2db179",
   "metadata": {},
   "source": [
    "## 4. Data Samplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72323e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize((CONFIG.IMG_SIZE, CONFIG.IMG_SIZE)),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "\n",
    "data_samplers = []\n",
    " \n",
    "\n",
    "for k in range(len(CONFIG.CLASSES)):\n",
    "    dataset = torchvision.datasets.MNIST(root=CONFIG.DATASET_PATH, download=True, \n",
    "                                         transform=transform)\n",
    "    idx = [t == CONFIG.CLASSES[k] for t in dataset.targets]\n",
    "    dataset.targets, dataset.data = np.array(dataset.targets)[idx], torch.tensor(dataset.data)[idx] \n",
    "    data_samplers.append(DatasetSampler(dataset,flag_label=True,batch_size =256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2db213",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samplers.append(color_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2020f675",
   "metadata": {},
   "source": [
    "## 5. Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385cc744",
   "metadata": {},
   "source": [
    "### 5.1 Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e53f1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = []\n",
    "encoder.append( ResNet_D(size=CONFIG.IMG_SIZE,\n",
    "                  nc=CONFIG.NC,\n",
    "                  nfilter=64, \n",
    "                  nfilter_max=512, \n",
    "                  res_ratio=0.1,\n",
    "                  n_output=2*CONFIG.LATENT_SIZE,bn_flag=True,pn_flag=True).to(CONFIG.DEVICE))\n",
    "            \n",
    "weights_init_D(encoder[0])\n",
    "\n",
    "encoder.append(linear_model(3,[64,128,256],2*CONFIG.LATENT_SIZE).to(CONFIG.DEVICE))\n",
    "param_enc = [net.parameters() for net in encoder]\n",
    " \n",
    "#param_enc = [net.parameters() for net in  encoder]\n",
    "encoder_opt = torch.optim.Adam(  itertools.chain(*param_enc),\n",
    "                                  CONFIG.LR_ENCODER, betas=CONFIG.BETAS)\n",
    "    \n",
    "latent_mlp, latent_mlp_opt = None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4875f583",
   "metadata": {},
   "source": [
    "### 5.2 Potentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9506a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nets_for_pot = [ResNet_D(size=CONFIG.IMG_SIZE,\n",
    "                  nc=3,\n",
    "                  nfilter=64, \n",
    "                  nfilter_max=512, \n",
    "                  res_ratio=0.1,\n",
    "                  n_output=1,bn_flag=False,pn_flag=False).to(CONFIG.DEVICE)\n",
    "                  ]\n",
    "\n",
    "\n",
    "for f in nets_for_pot: \n",
    "    weights_init_D(f)\n",
    "    \n",
    "nets_for_pot_opt = torch.optim.Adam( nets_for_pot[0].parameters(),\n",
    "                               CONFIG.LR_POTENTIAL, betas=CONFIG.BETAS)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e3332d",
   "metadata": {},
   "source": [
    "## 6.Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7b1929",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "name_exp = f\"KL_EPS_{EPSILON}_LMBD_{CONFIG.LAMBDAS}_mltp_{100}_THRESHOLD_{CONFIG.SATURATION_THRESHOLD}\"\n",
    "CONFIG.NAME_EXP = name_exp\n",
    "wandb.init(project=\"BNOT\" ,\n",
    "           name=name_exp,\n",
    "           config=CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94aab2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_shape_color_entropic( nets_for_pot,\n",
    "                    nets_for_pot_opt,\n",
    "                    encoder,\n",
    "                    encoder_opt,\n",
    "                    latent_mlp,\n",
    "                    latent_mlp_opt,\n",
    "                    data_samplers,\n",
    "                    generator=G,\n",
    "                    config=CONFIG\n",
    "                    \n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ad74e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b181f1ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8cac1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
